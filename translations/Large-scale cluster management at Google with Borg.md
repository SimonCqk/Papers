## 摘要

`Google Borg`是一个横跨多个集群（单个集群的机器数可达数万台），运行着成百上千不同应用任务的集群管理系统。
它通过进程级别的性能隔离，将准入控制，高效的任务打包，存储过度分配以及机器资源共享，糅合在一起以达到高利用率。它还具备最小化的故障恢复时间，
配置调度策略以降低相关故障率等运行时特性，来支持应用的高可用。`Borg`提供了描述式作业规范语言，命名（域名）服务集成，实时任务监控等系统行为分析与仿真工具来简化用户的工作。
我们将总结一下过去十余年对`Borg`系统的架构，特性，重要的设计决定，一些决策的分析和测试的经验。

## 1.介绍

我们内部称之为`Borg`的集群管理系统，管理，调度，启动，重启并监视了所有的`Google`应用。本文将会解释它是怎么做到的。
`Borg`提供了三大好处：（1）隐藏了资源管理和故障处理的细节，因此它的用户可以专注应用开发。（2）本身即高可靠/高可用，因此在其之上的应用也是。（3）让我们的工作负载高效地分摊到成千上万的机器上。`Borg`并不是第一个解决此类问题的系统，但它是极少数能够以如此高弹性和高完成度处理这种规模的系统之一。本文将围绕这些主题，通过我们十余年的经验得出一些定性的结论（观察）。

![figure 1](https://github.com/SimonCqk/Papers/blob/master/images/borg/f1.jpg)

## 2.用户角度

`Borg`的用户是`Google`应用及服务的开发者或SRE。用户以作业 **(jobs)** 的形式提交他们的任务 **(task)** ，每个job由一个或多个的二进制程序组成。每个job运行在单个`Borg` **cell** 上，管理的最小单元是一些机器的集合。这个章节将描述`Borg`展示给用户视角的一些主要特性。

### 2.1 Workload（工作负载）

`Borg cell`主要运行着两大异构的工作负载。第一部分是长时运行的服务（在线任务），主要处理短时且延迟敏感的请求（从几微秒到几百毫秒）。这些服务面向终端用户，例如`Gmail，Google Docs，Google search`，也用于内部的一些基础设施服务，例如`BigTable`。第二部分是处理时长从几秒到几天不等的批处理任务（离线任务），这些任务对短期的性能波动并不敏感。工作负荷在cells上的比例不同，依赖于它们的租期（某些cell的任务是高度密集的），并且在时间维度上这些任务的跨度也非常大：批处理任务来来去去，并且很多面向终端用户的服务处于昼夜不停的使用模式中。`Borg`需要平等地处理以上所有状况。
一个代表性的`Borg` 负载例子是一个公开的2011年5月整月的记录数据集，并且它已经得到了充分的分析。
过去几年已经有很多应用框架在`Borg`之上被建立起来了，包括我们内部的`MapReduce，FlumeJava，MillWheel，Pregel`等。这些中的大多数都有一个控制器来提交主job和一些工作job，`MapReduce`和`FlumeJava`扮演了与YARN应用管理者相似的角色。我们的分布式存储系统`GFS`，以及它的后继`CFS`，还有`BigTable，Megastore`都跑在`Borg`上。
在此文中，我们把高优先级的`Borg job`划分为"`production (prod) job`"，其余为"`non-production (non-prod) job`"。在一个具有代表性的cell中，`prod jobs`占据了大约70%的CPU资源以及大约60%的CPU利用率，同时占据了55%的总内存和85%的内存利用率。

### 2.2 Clusters and cells（集群和单元）

同一个cell里的机器都属于单个 **集群** ，通过数据中心级的高性能网络光缆连接在一起。一个集群存活在单个数据中心的建筑中，多个建筑构成了一个站点。一个集群通常只有一个cell，但是可能会有一些小型的用于测试的cell和其他有特殊用途的cell。我们竭尽所能地避免任何的单点故障。
我们的除去测试cell之后的中心cell规模大约有1万台机器，甚至有些更大。一个cell中的机器在多个维度都是异构的：大小（CPU，RAM，硬盘，网络），处理器类型，性能和IP地址的容量或闪存介质。`Borg`通过这些维度的差异将不同用户隔离开来并决定task被分配到哪个cell上运行，获取资源，安装程序及其依赖，监视它们的健康状况，并在他们异常退出时重启。

### 2.3 Jobs and tasks（作业和任务）

一个`Borg job`的属性包括名字，拥有者和它所包含的task数目。Jobs可以通过 **约束** 去强制它的tasks运行在特定 **属性** 的机器上，比如处理器架构，OS版本或是一个外网IP地址。约束可以是硬性的也可以是弹性的。弹性的指可以作为偏好设置而不是需求。一个job的启动可以延迟直到前一个job结束，一个job只能运行在一个cell中。
每个task映射到运行着容器的Linux进程集合中。绝大部分的`Borg`工作负载不运行在虚拟机上，因为我们不想承担硬件虚拟化的巨大开销。并且，这个系统是在我们很多处理器没有支持虚拟化的时候设计的。
`Task`也有它的属性，比如资源需求和它在job中的索引。同一个job中的大多数task有着相同的属性，但也可以被覆盖，例如通过特定任务的命令行参数。每个资源维度（CPU核数，RAM，硬盘空间，硬盘读写速率，TCP端口等）都被独立地指定。`Borg`程序为了减少对运行时环境的依赖，都采用静态链接，并且被组织成由二进制程序和数据文件组成的 **包** ，它们的安装都由`Borg`负责。
用户通过调用RPC来操作jobs，RPC通常通过命令行工具，其他`Borg` job或是监视系统来发出。大多数job的描述通过声明式配置语言BCI完成。BCI是GCL的变种，它会生成`protobuf`文件并通过`Borg`专有的关键字来拓展。GCL提供lambda函数来允许进行计算，可以被应用用来调整配置以适应环境。有数万的BCL文件超过1k行，累计已经超过1千万行的BCL。`Borg` job的配置和`Aurora`配置文件有很多相同之处。
图2展示了jobs和tasks在其生命周期内的状态转移。

![figure 2](https://github.com/SimonCqk/Papers/blob/master/images/borg/f2.jpg)

用户可以通过向`Borg`推送一份新的job配置文件来更改一个正在运行job中的部分或所有task，并且命令`Borg`将task **更新** 到最新。这些是一种轻量级的，非原子性的事务操作，那么它们在被关闭（提交）之前当然也可以轻易撤销。更新通常是滚动执行的，而且可以限制由更新导致的任务中断（被重新调度或抢占）的数量；超过限值后，任何将导致中断的改动都会被跳过。
一些task的更新操作（例如推送一个新的二进制文件）总是会触发重启，一些（例如对资源的需求数增长或改变了约束）会使得task不再适应于当前的机器，然后导致它被停止然后重新调度，还有一些（例如更改优先级）总是可以无需重启或移动而被完成。
Task可以请求在被kill之前通过`Unix SIGTERM`信号机制得到通知，从而有时间完成扫尾，保存状态，结束当前正在执行的请求并拒绝新请求等操作。如果抢占者设置了延迟上限那么有些通知将来不及发出。实践中，80%的情况下能发出通知。

### 2.4 Allocs（分配）

`Borg`上的 **Alloc** 是为一个或多个task运行的预留资源。不管有没有被使用，这些资源都被分配出去了。`Alloc`能为未来的task预留资源，在停止task与重启task之间保持资源，并能将不同job的task聚集到同一台机器上 —— 举个例子，一个web服务实例，以及一个与之关联的写log任务（将本地服务器的URL日志复制到分布式文件系统中）。`Alloc`像机器一样对待其管理的资源，多个运行在同一`Alloc`内的task共享其资源。如果一个`Alloc`需要被迁移到其他机器，那么从属于它的task都会被重新调度。
一个 **Alloc集合** 就像一个job：它是在一组在多台机器上预留了资源的allocs。一旦一个`Alloc`集合被创建，一个或多个job就可以被提交并运行。为了简洁起见，我们通常用"task"指代一个`Alloc`或一个顶级task（独立于alloc），用"job"指代一个job或是`Alloc`集合。

### 2.5 Priority, quota, and admission control（优先级，配额和准入控制）

当更多可被容纳的工作出现时（注：即超过了系统的负载能力）会发生什么？我们的解决方案是引入优先级和配额。
每个job都有 **优先级** ，一个正整数。一个高优先级的task可以通过牺牲低优先级的task来获取资源，甚至可以抢占（杀死）后者。`Borg`为不同的用途定义了不重叠的 **优先级区间** ，包括（以降序排列）： **监控，生产，批处理，最大限度（测试的或免费的）** 。在本文中，`prod jobs`指的就是监控和生产区间。
尽管一个被抢占的task通常会被重新调度到cell的某个机器上，级联抢占还是会发生的，例如当一个高优先级task抢占了一个稍低优先级的task，而这个task又抢占了更低优先级的另一个task。为了消除这样的情况，我们禁止`production`区间内的task之间进行抢占。细粒度的优先级划分在别的情况下也同样有用。例如`MapReduce`的`master`上的task比`worker`节点上的task优先级稍高一些，为的就是提高`master`的可靠性。优先级直接反映了cell中正在运行或者正在等待的job的重要程度。 **配额(Quota)** 被用于调度哪个job **准入** 。在一个给定的优先级的一段时间内（通常是数月），配额用一个资源向量（CPU，RAM，硬盘灯）来表示某时用户job某时可以请求的最大资源（例如，在cell xx，`prod`优先级下，20TB的RAM，直到7月底）。配额检查是准入控制的一部分，而非调度：当job的配额不足时会在提交时被立刻拒绝。
高优先级的配额比低优先级的配额开销更大。`Production`级别的配额会被cell中的实际可用资源所限制，所以当一个用户提交了`production`优先级的job且此job满足配额时，它一定可以运行。尽管我们鼓励用户按需购买配额，很多用户还是会超买以防未来应用基数增长时将要面临的资源短缺。我们对低优先级的配额超卖来应对这样的情况，这意味着当优先级为0时用户有无限的配额，尽管这几乎无法实现。一个低优先级的job可以准入，但是因为资源不足会保持等待。
配额是`Borg`之外的系统处理的，并且和我们的物理容量规划密切相关，在不同的数据中心，容量规划反映出的价格和可用性都会不尽相同。用户的job只有在请求的优先级上有足够配额时才会被接收。配额的使用使得主导资源公平性（DRF）[29, 35, 36, 66]变得不必要。
`Borg`有一个可以为某些用户提供特殊权限的容量系统，例如，允许管理员删除或更改cell中的任意job，或者允许用户访问被限制的内核特性和禁用job资源预估这样的`Borg`行为。

### 2.6 Naming and monitoring（命名和监控）

仅仅创建和部署tasks是不够的：即使tasks被迁移到了新机器上，服务的客户端和其他系统也应该能找到他们。为了实现这个特性，`Borg`建立了一个稳定的“`Borg`域名服务”为每个task命名，包括cell名，job名和task编号。`Borg`把一个task的主机名和端口号写入一个持久化存储且高可用的`Chubby`文件中，这被`RPC`系统用来找到task的终端（真实所在的位置）。`BNS`域名也构成了task的`DNS`域名的基础，例如，cell cc中urban用户的名为jfoo的job下的第50个task会被命名为50.jfoo.ubar.cc.borg.google.com。当job的大小和task的健康状况改变时，`Borg`会写入`Chubby`，因此负载均衡方可以看到请求被路由到了哪里。
几乎所有运行在`Borg`之下的task都会包含一个内置的HTTP服务，它会发布task的健康信息以及数千的性能指标（例如，RPC延迟）。`Borg`通过健康检查URL监控task并对那些没有及时相应或返回了HTTP错误码的task进行重启。其他数据背会监控工具跟踪并展示在仪表盘上，当超出服务水平目标（SLO）时会报警。
一个被称为`Sigma`的服务提供了基于web的用户界面（UI），用户可以检测他们所有job的状态，或是一个特性的cell，或者慎入研究独立jobs和tasks的资源状况，日志细节，执行历史和最终的结果。我们的应用会产生大量的日志，它们会自动的轮转以防用尽磁盘空间，并且在退出后会保留一段时间，为了辅助调试。如果一个job没有处于运行中，`Borg`会提供一个"why pending？（为什么挂起？）"的注解，以及一个如何修改job的资源需求以更好适应cell的指引。我们发布了一个“符合”资源形态使得调度更加容易的指南。
`Borg`会把所有的job提交记录和task事件，以及每个task尽可能详细的资源使用信息记录在`Infrastore`上，`Infrastore`是一个可扩展的只读数据存储，通过`Dremel`提供了类SQL的交互式接口。这些数据被用来支持基于使用量的收费，调试job和系统错误以及长期的容量规划。它也为`Google`集群的工作负载追踪提供了数据。
所有的这些特性帮助用户理解`Borg`的行为，调试job，协助SRE可以独立管理成千上万的机器。

## Borg architecture（Borg架构）

一个`Borg cell`由一些机器，一个被称为`Borgmaster`的逻辑上的中心控制器，以及一个被称为`Borglet`的运行在cell的每台机器上的一个代理进程。`Borg`的组件都是用C++写的。

### 3.1 Borgmaster

每个cell的`Borgmaster`都由两个进程构成：一个主进程和一个独立的调度进程。`Borgmaster`的主进程处理来自客户端的RPC请求，如修改状态（例如创建job），提供只读的数据访问（例如查询job）。它还管理了系统中所有对象（机器，tasks，分配等）的状态机，通过`Borglets`的通信，并提供一个web UI作为`Sigma`的备份。
`Borgmaster`逻辑上是一个单进程，但实际上有5个副本。每个副本在内存中维护了一份cell中大多数状态的拷贝，并且这个状态会被记录在一个高可用，分布式且基于`Paxos`的存储中(副本的本地硬盘上)。每个cell上仅有一个 **选举出的master** ，同时作为`Paxos`集群的leader和状态的修改者，处理所有改变cell状态的操作，例如提交一个job或终止一台机器上的某个task。当一个cell启动或之前选举出的master宕机了，都会触发一次master节点的选举（利用`Paxos`协议），新的master会获取一个`Chubby`锁，这样别的系统就能找到它。选举一个新master并把故障转移通常需要10s，但是在一个大型cell中耗时也能高达1分钟，因为内存中的状态需要被重新构建。当一个副本从宕机中恢复过来时，它会动态地从其他最新的`Paxos`副本中重新同步它的状态。
`Borgmaster`某个时间点的状态被称为检查点（checkpoint），以定期的快照（snapshot）加上改动日志的形式持久化在`Paxos`存储中。检查点有很多用途，包括恢复过去任意一个时间点的`Borgmaster`的状态（例如，把时间点设为接受某个触发软件缺陷的请求之前，这样可以用于调试）； **极端情况** 下可以手动修复它；构建事件的持久化日志，用于未来的查询；还有离线仿真。
一个叫作`Fauxmaster`的高保真`Borgmaster`仿真器被用来读取检查点文件，并且包含了一份完整的线上`Borgmaster`代码，还有`Borglets`的存根接口。它接收`RPC`请求去改动状态机，并执行类似“调度所有挂起的task”这样的操作。我们还用`Fauxmaster`来调试故障，把它当做一个真实的`Borgmaster`来与它交互，用模拟的`Borglets`从检查点文件中重放真实的交互操作。用户可以进入系统一步一步观察过去发生的状态改变。`Fauxmaster`对于容量规划也很有帮助（多少同样类型的新job会合适？），还有在对cell的配置改动之前进行可行性检查（这样的改动会导致任何重要的job不可用吗？）

### 3.2 Scheduling（调度）

当一个job被提交时，`Borgmaster`会将它持久化在`Paxos`的存储中，并且将job里的tasks加入 **挂起（等待）队列** 中。 **调度器** 会异步地扫描队列，如果有足够的资源且符合job的约束，它会将tasks赋给机器。（调度器主要操作tasks而不是jobs）。扫描会遵循从高到低的优先级，同一级别优先级下的会采用轮转法来保证不同用户之间的公平性，以避免队首大型任务阻塞。调度算法包括两个部分： **可行性检查（feasibility checking）** ，找到task可以运行的机器集合，和 **评分（scoring）** ，从可行的机器中选择一台。
在可行性检查中，调度器找到满足tasks约束并且有足够多可用资源的机器集合（包括那些可以从低优先级任务中抢夺过来的资源）。在评分阶段，调度器会判断每个可行机器的质量。评分考虑了用户指定的一些偏好，但是主要还是由内置的标准驱动，例如最小化被抢占任务的优先级和数量，选择已经有任务安装包的机器，使任务分散在不同能耗和故障域之间，以及整合质量（在单台机器上混合高低优先级的tasks使得高优先级任务可以在负载尖峰扩容）等。
`Borg`之前使用`E-PVM`算法用来评分，`E-PVM`对异构资源生成等效的成本值，最小化了部署一个任务的成本。在实践中，`E-PVM`将负载传播到了所有的机器上，为负载尖峰留出了资源——但代价是增加了碎片，尤其是对需要占据一台机器大部分资源的大型task来说。有时候我们称之为“最差匹配（worst fit）”。
与之相反的是“最佳匹配（best fit）”。“最佳匹配”尽可能的将任务填满机器。这使得某些机器没有在运行用户的job（但仍然运行着存储服务），因此部署大型任务就会非常直接，但是如果`Borg`或用户误判了资源的需求，这样紧实的任务整合策略会导致性能受损，当有突发的高负载时会损害应用，尤其是对指定了低CPU需求的批处理job有负面影响，它们本应能轻易被调度并且抓住机会使用空闲的资源：20%的`non-prod`优先级的tasks只请求了少于0.1核的CPU。
我们当前的评分模型是一个混合模型，力图减少搁浅（stranded）资源的数量——搁浅资源指的是某些因为其他类型的资源被饱和分配了而不能使用的资源。它比最佳匹配模型高出了大约3-5%的整合效率。
如果在评分阶段被选中的机器没有足够的可用资源去匹配新task，`Borg`会 **抢占（杀死）** 更低优先级的tasks，优先级从低到高直到有足够的可用资源。我们将被抢占的tasks加入调度器的挂起队列中，而不是将它迁移或休眠。
Task的启动延迟（从job提交到task运行所经历的时间）是我们持续关注的重点。它是一个变化很大的值，中位数大约为25s。包的安装大约占据了总时间的80%：一个已知的性能瓶颈是不同的包在写入本地硬盘时会产生竞争。为了减少task的启动时间，调度器更倾向于把task赋给已经有必要安装包（程序和数据）的机器：大多数的安装包是只读的因此可以被共享和缓存。（这是`Borg`调度器唯一支持的一种数据局部化形式。）除此之外，`Borg`利用树形结构和类似torrent的协议并行地把包分发到不同机器上。
此外，调度器使用了一些技术使得它可以扩展到包含数万台机器的cell中。

### 3.3 Borglet

`Borglet`是存在于cell的每台机器上的本地`Borg`代理。它被用来启动和停止tasks，重启失败的task，通过内核调用来管理本地资源，滚动吊事日志，并且向`Borgmaster`和其他监控系统报告机器的状态。
`Borgmaster`每隔几秒会轮询每个`Borglet`来获取机器的当前状态并发送请求。这使得`Borgmaster`可以控制通信的速率，避免了复杂的流量控制机制，且防止了恢复风暴。
选举出的master节点负责准备发往`Borglet`的消息，并根据他们的响应更新cell的状态。为了性能的可拓展性，每个`Borgmaster`的副本运行着一个无状态的 **链接分片（link shard）** 去处理和某些`Borglet`的通信；只要新的选举发生，这样的分区就会重新被重新计算。为了容错性，`Borglet`会一直报告它的完整状态，链接分片会把这些信息聚合并压缩，只向状态机报告与之前的不同之处，以此减少`master`节点的更新负载。
如果一个`Borglet`对多次的轮询信息都没有作出响应，这台机器就会被标记为宕机，其上的所有task都会被重新调度。如果通信恢复，`Borgmaster`为避免复制的开销，会通知`Borglet`杀死那些被重新调度task。即使和`Borgmaster`失去联系，`Borglet`也会继续执行普通的操作，因此当前正在运行的task和服务会被保留就算`Borgsmaster`复制失败了。