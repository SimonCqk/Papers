## 摘要

`Google Borg`是一个横跨多个集群（单个集群的机器数可达数万台），运行着成百上千不同应用任务的集群管理系统。
它通过进程级别的性能隔离，将准入控制，高效的任务打包，存储过度分配以及机器资源共享，糅合在一起以达到高利用率。它还具备最小化的故障恢复时间，
配置调度策略以降低相关故障率等运行时特性，来支持应用的高可用。`Borg`提供了描述式作业规范语言，命名（域名）服务集成，实时任务监控等系统行为分析与仿真工具来简化用户的工作。
我们将总结一下过去十余年对`Borg`系统的架构，特性，重要的设计决定，一些决策的分析和测试的经验。

## 1.介绍

我们内部称之为`Borg`的集群管理系统，管理，调度，启动，重启并监视了所有的`Google`应用。本文将会解释它是怎么做到的。
`Borg`提供了三大好处：（1）隐藏了资源管理和故障处理的细节，因此它的用户可以专注应用开发。（2）本身即高可靠/高可用，因此在其之上的应用也是。（3）让我们的工作负载高效地分摊到成千上万的机器上。`Borg`并不是第一个解决此类问题的系统，但它是极少数能够以如此高弹性和高完成度处理这种规模的系统之一。本文将围绕这些主题，通过我们十余年的经验得出一些定性的结论（观察）。

## 2.用户角度

`Borg`的用户是`Google`应用及服务的开发者或SRE。用户以作业`(jobs)`的形式提交他们的任务`(task)`，每个job由一个或多个的二进制程序组成。每个job运行在单个`Borg cell`上，管理的最小单元是一些机器的集合。这个章节将描述Borg展示给用户视角的一些主要特性。

### 2.1 Workload（工作负载）

`Borg cell`主要运行着两大异构的工作负载。第一部分是长时运行的服务（在线任务），主要处理短时且延迟敏感的请求（从几微秒到几百毫秒）。这些服务面向终端用户，例如`Gmail，Google Docs，Google search`，也用于内部的一些基础设施服务，例如`BigTable`。第二部分是处理时长从几秒到几天不等的批处理任务（离线任务），这些任务对短期的性能波动并不敏感。工作负荷在cells上的比例不同，依赖于它们的租期（某些cell的任务是高度密集的），并且在时间维度上这些任务的跨度也非常大：批处理任务来来去去，并且很多面向终端用户的服务处于昼夜不停的使用模式中。Borg需要平等地处理以上所有状况。
一个代表性的`Borg` 负载例子是一个公开的2011年5月整月的记录数据集，并且它已经得到了充分的分析。
过去几年已经有很多应用框架在`Borg`之上被建立起来了，包括我们内部的`MapReduce，FlumeJava，MillWheel，Pregel`等。这些中的大多数都有一个控制器来提交主job和一些工作job，`MapReduce`和`FlumeJava`扮演了与YARN应用管理者相似的角色。我们的分布式存储系统`GFS`，以及它的后继`CFS`，还有`BigTable，Megastore`都跑在`Borg`上。
在此文中，我们把高优先级的`Borg job`划分为"`production (prod) job`"，其余为"`non-production (non-prod) job`"。在一个具有代表性的cell中，`prod jobs`占据了大约70%的CPU资源以及大约60%的CPU利用率，同时占据了55%的总内存和85%的内存利用率。

### 2.2 Clusters and cells（集群和单元）

同一个cell里的机器都属于单个集群，通过数据中心级的高性能网络光缆连接在一起。一个集群存活在单个数据中心的建筑中，多个建筑构成了一个站点。一个集群通常只有一个cell，但是可能会有一些小型的用于测试的cell和其他有特殊用途的cell。我们竭尽所能地避免任何的单点故障。
我们的除去测试cell之后的中心cell规模大约有1万台机器，甚至有些更大。一个cell中的机器在多个维度都是异构的：大小（CPU，RAM，硬盘，网络），处理器类型，性能和IP地址的容量或闪存介质。`Borg`通过这些维度的差异将不同用户隔离开来并决定task被分配到哪个cell上运行，获取资源，安装程序及其依赖，监视它们的健康状况，并在他们异常退出时重启。

### 2.3 Jobs and tasks（作业和任务）

一个`Borg job`的属性包括名字，拥有者和它所包含的task数目。Jobs可以通过约束去强制它的tasks运行在特定属性的机器上，比如处理器架构，OS版本或是一个外网IP地址。约束可以是硬性的也可以是弹性的。弹性的指可以作为偏好设置而不是需求。一个job的启动可以延迟直到前一个job结束，一个job只能运行在一个cell中。
每个task映射到运行着容器的Linux进程集合中。绝大部分的Borg工作负载不运行在虚拟机上，因为我们不想承担硬件虚拟化的巨大开销。并且，这个系统是在我们很多处理器没有支持虚拟化的时候设计的。
`Task`也有它的属性，比如资源需求和它在job中的索引。同一个job中的大多数task有着相同的属性，但也可以被覆盖，例如通过特定任务的命令行参数。每个资源维度（CPU核数，RAM，硬盘空间，硬盘读写速率，TCP端口等）都被独立地指定。`Borg`程序为了减少对运行时环境的依赖，都采用静态链接，并且被组织成由二进制程序和数据文件组成的包，它们的安装都由`Borg`负责。
用户通过调用RPC来操作jobs，RPC通常通过命令行工具，其他`Borg` job或是监视系统来发出。大多数job的描述通过声明式配置语言BCI完成。BCI是GCL的变种，它会生成`protobuf`文件并通过`Borg`专有的关键字来拓展。GCL提供lambda函数来允许进行计算，可以被应用用来调整配置以适应环境。有数万的BCL文件超过1k行，累计已经超过1千万行的BCL。`Borg` job的配置和Aurora配置文件有很多相同之处。
用户可以通过向Borg推送一份新的job配置文件来更改一个正在运行job中的部分或所有task，并且命令`Borg`将task更新到最新。这些是一种轻量级的，非原子性的事务操作，那么它们在被关闭（提交）之前当然也可以轻易撤销。更新通常是滚动执行的，而且可以限制由更新导致的任务中断（被重新调度或抢占）的数量；超过限值后，任何将导致中断的改动都会被跳过。
一些task的更新操作（例如推送一个新的二进制文件）总是会触发重启，一些（例如对资源的需求数增长或改变了约束）会使得task不再适应于当前的机器，然后导致它被停止然后重新调度，还有一些（例如更改优先级）总是可以无需重启或移动而被完成。
Task可以请求在被kill之前通过`Unix SIGTERM`信号机制得到通知，从而有时间完成扫尾，保存状态，结束当前正在执行的请求并拒绝新请求等操作。如果抢占者设置了延迟上限那么有些通知将来不及发出。实践中，80%的情况下能发出通知。

### 2.4 Allocs（分配）

`Borg`上的`Alloc`是为一个或多个task运行的预留资源。不管有没有被使用，这些资源都被分配出去了。`Alloc`能为未来的task预留资源，在停止task与重启task之间保持资源，并能将不同job的task聚集到同一台机器上 —— 举个例子，一个web服务实例，以及一个与之关联的写log任务（将本地服务器的URL日志复制到分布式文件系统中）。`Alloc`像机器一样对待其管理的资源，多个运行在同一`Alloc`内的task共享其资源。如果一个`Alloc`需要被迁移到其他机器，那么从属于它的task都会被重新调度。
一个`Alloc`集合就像一个job：它是在一组在多台机器上预留了资源的allocs。一旦一个`Alloc`集合被创建，一个或多个job就可以被提交并运行。为了简洁起见，我们通常用"task"指代一个`Alloc`或一个顶级task（独立于alloc），用"job"指代一个job或是`Alloc`集合。

### 2.5 Priority, quota, and admission control（优先级，配额和准入控制）

当更多可被容纳的工作出现时（注：即超过了系统的负载能力）会发生什么？我们的解决方案是引入优先级和配额。
每个job都有优先级，一个正整数。一个高优先级的task可以通过牺牲低优先级的task来获取资源，甚至可以抢占（杀死）后者。`Borg`为不同的用途定义了不重叠的优先级区间，包括（以降序排列）：监控，生产，批处理，最大限度（测试的或免费的）。在本文中，`prod jobs`指的就是监控和生产区间。
尽管一个被抢占的task通常会被重新调度到cell的某个机器上，级联抢占还是会发生的，例如当一个高优先级task抢占了一个稍低优先级的task，而这个task又抢占了更低优先级的另一个task。为了消除这样的情况，我们禁止`production`区间内的task之间进行抢占。细粒度的优先级划分在别的情况下也同样有用。例如`MapReduce`的`master`上的task比`worker`节点上的task优先级稍高一些，为的就是提高`master`的可靠性。优先级直接反映了cell中正在运行或者正在等待的job的重要程度。配额(`Quota`)被用于调度哪个job准入。在一个给定的优先级的一段时间内（通常是数月），配额用一个资源向量（CPU，RAM，硬盘灯）来表示某时用户job某时可以请求的最大资源（例如，在cell xx，`prod`优先级下，20TB的RAM，直到7月底）。配额检查是准入控制的一部分，而非调度：当job的配额不足时会在提交时被立刻拒绝。
高优先级的配额比低优先级的配额开销更大。`Production`级别的配额会被cell中的实际可用资源所限制，所以当一个用户提交了`production`优先级的job且此job满足配额时，它一定可以运行。尽管我们鼓励用户按需购买配额，很多用户还是会超买以防未来应用基数增长时将要面临的资源短缺。我们对低优先级的配额超卖来应对这样的情况，这意味着当优先级为0时用户有无限的配额，尽管这几乎无法实现。一个低优先级的job可以准入，但是因为资源不足会保持等待。
配额是`Borg`之外的系统处理的，并且和我们的物理容量规划密切相关，在不同的数据中心，容量规划反映出的价格和可用性都会不尽相同。用户的job只有在请求的优先级上有足够配额时才会被接收。配额的使用使得主导资源公平性（DRF）[29, 35, 36, 66]变得不必要。
`Borg`有一个可以为某些用户提供特殊权限的容量系统，例如，允许管理员删除或更改cell中的任意job，或者允许用户访问被限制的内核特性和禁用job资源预估这样的`Borg`行为。

### 2.6 Naming and monitoring（命名和监控）

仅仅创建和部署tasks是不够的：即使tasks被迁移到了新机器上，服务的客户端和其他系统也应该能找到他们。为了实现这个特性，`Borg`建立了一个稳定的“`Borg`域名服务”为每个task命名，包括cell名，job名和task编号。`Borg`把一个task的主机名和端口号写入一个持久化存储且高可用的`Chubby`文件中，这被`RPC`系统用来找到task的终端（真实所在的位置）。`BNS`域名也构成了task的`DNS`域名的基础，例如，cell cc中urban用户的名为jfoo的job下的第50个task会被命名为50.jfoo.ubar.cc.borg.google.com。当job的大小和task的健康状况改变时，`Borg`会写入`Chubby`，因此负载均衡方可以看到请求被路由到了哪里。
几乎所有运行在`Borg`之下的task都会包含一个内置的HTTP服务，它会发布task的健康信息以及数千的性能指标（例如，RPC延迟）。`Borg`通过健康检查URL监控task并对那些没有及时相应或返回了HTTP错误码的task进行重启。其他数据背会监控工具跟踪并展示在仪表盘上，当超出服务水平目标（SLO）时会报警。
一个被称为`Sigma`的服务提供了基于web的用户界面（UI），用户可以检测他们所有job的状态，或是一个特性的cell，或者慎入研究独立jobs和tasks的资源状况，日志细节，执行历史和最终的结果。我们的应用会产生大量的日志，它们会自动的轮转以防用尽磁盘空间，并且在退出后会保留一段时间，为了辅助调试。如果一个job没有处于运行中，`Borg`会提供一个"why pending？（为什么挂起？）"的注解，以及一个如何修改job的资源需求以更好适应cell的指引。我们发布了一个“符合”资源形态使得调度更加容易的指南。
`Borg`会把所有的job提交记录和task事件，以及每个task尽可能详细的资源使用信息记录在`Infrastore`上，`Infrastore`是一个可扩展的只读数据存储，通过`Dremel`提供了类SQL的交互式接口。这些数据被用来支持基于使用量的收费，调试job和系统错误以及长期的容量规划。它也为`Google`集群的工作负载追踪提供了数据。
所有的这些特性帮助用户理解`Borg`的行为，调试job，协助SRE可以独立管理成千上万的机器。

## Borg architecture（Borg架构）